{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VaryMinions.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNihTLU/gbotdCUfysaW/xj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfortz/VaryMinions/blob/jupyter-notebook/VaryMinions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77MW3TcZgP1E"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGLglA-0gMAm",
        "outputId": "ccefd1c0-dd18-4cf1-ce1c-238e492ca4b2"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import argparse\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import tensorflow as tf\n",
        "import keras  # this activates tensorflow backend directly - do not remove\n",
        "from sklearn.model_selection import train_test_split\n",
        "from os import path\n",
        "from os import environ\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from tensorflow.keras import backend as kb\n",
        "from tensorflow.keras.callbacks import TerminateOnNaN\n",
        "from tensorflow import test as tf_test\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "drive_dir = '/content/drive/My Drive/VaryMinions-Claroline/'"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie30xEsiPdlv"
      },
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLvIbfTlMYB9"
      },
      "source": [
        "# preprocessing to a multi-labelled dataset\n",
        "def multi_labelled(file_name, data):\n",
        "    dir = '../../datasets/multi-label/'\n",
        "    file = drive_dir + file_name\n",
        "    classes_single = data.filter(regex=\"Category.*\")\n",
        "    x = data.drop(classes_single.columns, axis=1)\n",
        "    x_no_dup = x.drop_duplicates()\n",
        "    xy_no_dup = data.drop_duplicates()\n",
        "    xy_only_dup = xy_no_dup.drop(x_no_dup.index)\n",
        "    cl_decod, df_cl_encoded = preprocessing_cat(classes_single)\n",
        "    classes_multi = pd.DataFrame()\n",
        "\n",
        "    for ind, tr in x_no_dup.iterrows():\n",
        "        out = df_cl_encoded.loc[ind]\n",
        "        result = out\n",
        "        for ind2, y2 in xy_only_dup.iterrows():\n",
        "            tr2 = x.loc[ind2]\n",
        "            out2 = df_cl_encoded.loc[ind2]\n",
        "            if tr.equals(other=tr2) & (not (out.equals(other=out2))):\n",
        "                result = result.add(out2)\n",
        "        classes_multi = classes_multi.append(result, ignore_index=True)\n",
        "\n",
        "    cl_names = cl_decod.inverse_transform(np.diag(classes_multi.columns.values))\n",
        "    classes_multi.columns = cl_names\n",
        "    classes_multi = classes_multi.add_prefix(\"Category_\")\n",
        "    data_multi = classes_multi.join(x_no_dup.reset_index(drop=True))\n",
        "    data_multi.to_csv(file, encoding='utf-8', index=False)\n",
        "\n",
        "\n",
        "# load dataset\n",
        "def load_dataset(dataset_filename, multi_label):\n",
        "    # load dataset .csv according to first argument\n",
        "    if multi_label:\n",
        "        dataset_dir = drive_dir #'../../datasets/multi-label/'\n",
        "    else:\n",
        "        dataset_dir = '../../datasets/single-label/'\n",
        "\n",
        "    file_to_load = dataset_dir + dataset_filename\n",
        "    print(f\"Trying to load: {file_to_load}\")\n",
        "    if not path.exists(dataset_dir):\n",
        "        print(f\"The expected dataset directory {dataset_dir} was not found.  Multi-labelling is: {multi_label}\")\n",
        "        sys.exit()\n",
        "    elif (not path.exists(str(dataset_dir + '/' + dataset_filename))) & multi_label:\n",
        "        print(\"There is no dataset for multi-labelled data. Trying to generate it from a single-labelled \"\n",
        "              \"corresponding dataset...\")\n",
        "        single_dataset = load_dataset(dataset_filename, False)\n",
        "        print(\"Single-labelled dataset found. Creating the multi-labelled version...\")\n",
        "        multi_labelled(dataset_filename, single_dataset)\n",
        "        print(\"File \" + file_to_load + \" created successfully.\")\n",
        "    elif not path.exists(file_to_load):\n",
        "        print(file_to_load)\n",
        "        print(\"The dataset file was not found\")\n",
        "        sys.exit()\n",
        "\n",
        "    # dealing with process of different lengths by filling missing values\n",
        "    dataset = np.genfromtxt(file_to_load, delimiter=',', missing_values='', filling_values='', skip_header=0,\n",
        "                            names=True, dtype=None, encoding=\"utf-8\")\n",
        "    pd_dataset = pd.DataFrame(dataset)\n",
        "    return pd_dataset\n",
        "\n",
        "\n",
        "# retrieve unique values in all dimensions of a dataset -> that would be the 'alphabet' of possible events\n",
        "def retrieve_unique_values(dataset):\n",
        "    flat_dataset = dataset.values.flatten()\n",
        "    unique_value = list(set(flat_dataset))\n",
        "    # print(len(unique_value))\n",
        "    # print(unique_value)\n",
        "    return unique_value\n",
        "\n",
        "\n",
        "# preprocessing dataset\n",
        "def preprocessing_to_num(dataset):\n",
        "    values = retrieve_unique_values(dataset)\n",
        "\n",
        "    # Creating a dictionary that maps integers to the events/actions (adapted from https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)\n",
        "    int2event = dict(enumerate(values))\n",
        "    # Creating another dictionary that maps events/actions to integers (adapted from https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)\n",
        "    event2int = {char: ind for ind, char in int2event.items()}\n",
        "    # print(event2int)\n",
        "\n",
        "    # encoding events with mapping dictionary\n",
        "    encoded_events = []\n",
        "    # print(range(len(dataset)))\n",
        "    for i, row in dataset.iterrows():\n",
        "        encoded_events.append(row)\n",
        "\n",
        "    for i, row in dataset.iterrows():\n",
        "        encoded_events[i] = [event2int[event] for event in row]\n",
        "\n",
        "    return encoded_events, event2int, int2event\n",
        "\n",
        "\n",
        "# preprocessing categories\n",
        "def preprocessing_cat(categories):\n",
        "    lb = preprocessing.LabelBinarizer()\n",
        "    lb.fit(np.array(categories))\n",
        "\n",
        "    print(\"Categories:\")\n",
        "    print(lb.classes_)\n",
        "    cl_enc = lb.transform(categories)\n",
        "\n",
        "    return lb, pd.DataFrame(cl_enc)\n",
        "\n",
        "\n",
        "# preprocessing\n",
        "def preproc(dataset_filename, multi):\n",
        "    df_dataset = load_dataset(dataset_filename, multi)\n",
        "    classes = df_dataset.filter(regex=\"Category.*\")\n",
        "    df_dataset = df_dataset.drop(classes.columns, axis=1)\n",
        "    df_cl_decod, df_cl_encoded = preprocessing_cat(classes)\n",
        "    encoded_ev, event2int, int2event = preprocessing_to_num(df_dataset)\n",
        "    df_ev_encoded = pd.DataFrame(encoded_ev)\n",
        "    return df_ev_encoded, df_cl_encoded, event2int, int2event\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN1iUpE-P-E1"
      },
      "source": [
        "## **Losses**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvLjTHYCP78I"
      },
      "source": [
        "# used for segementation, not sure that what we want here :(\n",
        "def ioU_jaccard_distance(y_true, y_pred, smooth=100):\n",
        "    \"\"\" This distance is copied verbatim from:\n",
        "    https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/jaccard.py\"\"\"\n",
        "\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
        "    print(\"intersection\")\n",
        "    kb.print_tensor(intersection)\n",
        "    print(\"union\")\n",
        "    kb.print_tensor(sum_)\n",
        "\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    return (1 - jac) * smooth\n",
        "\n",
        "\n",
        "def dummy_indicator(y_predicted, thres=0.0):\n",
        "    \"\"\"\n",
        "    This indicator return a tensor with values 0 if values in y_predicted < thres, else 1.\n",
        "    \"\"\"\n",
        "\n",
        "    y_predicted = K.tf.where(K.greater(y_predicted, thres),K.ones_like(y_predicted), K.zeros_like(y_predicted))\n",
        "\n",
        "    return y_predicted\n",
        "\n",
        "# TODO: remove this method not suitable as a loss\n",
        "def vary_boolean_jaccard(y_true, y_predicted):\n",
        "    \"\"\"\n",
        "    This function implements the classic jaccard distance over an indicator functions that converts float predictions\n",
        "    into boolean values.\n",
        "    \"\"\"\n",
        "    y_predicted = dummy_indicator(y_predicted)\n",
        "    y_true = K.cast(y_true, tf.float32)\n",
        "    intersection = K.sum(K.abs(y_true - y_predicted), axis=-1)\n",
        "    union = K.sum(K.ones_like(y_predicted), axis=-1) # number of possible matches (tensors y_predicted, y_true have the same number\n",
        "    # of values)\n",
        "    j_index = K.tf.divide(intersection, union)\n",
        "\n",
        "    return j_index\n",
        "\n",
        "# a different form of the jaccard distance which considers minimum and maximum values of the tensors \n",
        "def vary_weighted_jaccard(y_true, y_predicted):\n",
        "    \"\"\" This function implements the weighted jaccard distance also known as Soergel:\n",
        "    https://en.wikipedia.org/wiki/Jaccard_index. Since it works on real and positive numbers no indicator is needed\n",
        "    to translate probabilities into Boolean values. \"\"\"\n",
        "\n",
        "    # we convert actual labels to float to ease comparisons\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    num = kb.sum(kb.minimum(y_true, y_predicted), axis=-1)\n",
        "    denom = kb.sum(kb.maximum(y_true, y_predicted), axis=-1)\n",
        "    j_index = tf.divide(num, denom)\n",
        "\n",
        "    ones = tf.ones_like(j_index)\n",
        "    return ones - j_index\n",
        "\n",
        "# bounded version of the weighted jaccard distance putting negative values to 0\n",
        "def vary_weighted_jaccard_rectified(y_true, y_predicted):\n",
        "    \"\"\" This function implements the weighted jaccard distance also known as Soergel:\n",
        "    https://en.wikipedia.org/wiki/Jaccard_index. Since it works on real and positive numbers no indicator is needed\n",
        "    to translate probabilities into Boolean values. This function \"rectifies\" negative values to zero. \"\"\"\n",
        "\n",
        "    # rectification of negative values to zero\n",
        "    zeros = tf.zeros_like(y_predicted)\n",
        "    y_predicted = kb.maximum(y_predicted, zeros)\n",
        "    # we convert actual labels to float to ease comparisons\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "\n",
        "    num = kb.sum(kb.minimum(y_true, y_predicted), axis=-1)\n",
        "    denom = kb.sum(kb.maximum(y_true, y_predicted), axis=-1)\n",
        "    j_index = tf.divide(num, denom)\n",
        "\n",
        "    ones = tf.ones_like(j_index)\n",
        "    return ones - j_index\n",
        "\n",
        "# the manhattan distance between two tensors\n",
        "def vary_manhattan_dist(y_true,y_predicted):\n",
        "    \"\"\"\n",
        "    Implements Manatthan distance a loss\n",
        "    \"\"\"\n",
        "    y_actual_float = tf.cast(y_true, tf.float32)\n",
        "    manh_dist = kb.sum(kb.abs(y_actual_float - y_predicted), axis=-1)\n",
        "    return manh_dist\n",
        "\n",
        "# TODO remove this method\n",
        "def vary_manhattan_dist_indiv(y_true,y_predicted):\n",
        "    \"\"\"\n",
        "    Implements Manatthan distance a loss. Reports detailed losses per classes. DO NOT USE\n",
        "    \"\"\"\n",
        "    y_actual_float = tf.cast(y_true, tf.float32)\n",
        "    manh_dist = kb.abs(y_actual_float - y_predicted)\n",
        "    return manh_dist\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVYgkVYEObSw"
      },
      "source": [
        "## **Training Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acKoqTl5PriL"
      },
      "source": [
        "### **Training LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNROM35APqMf"
      },
      "source": [
        "def get_LSTM_model(multi=False, alpha_size=128, nb_classes=1, nb_col=128, nb_unit=10, activation='tanh', loss='mse'):\n",
        "    \n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Embedding(alpha_size, alpha_size, mask_zero=True))\n",
        "    #model.add(layers.Bidirectional(layers.LSTM(nb_unit, activation='relu', recurrent_activation = 'sigmoid')))\n",
        "    model.add(layers.Bidirectional(layers.LSTM(nb_unit, activation='tanh', recurrent_activation = 'sigmoid', use_bias=True, recurrent_dropout=0.0, unroll=False)))\n",
        "###\n",
        "    if multi:\n",
        "\n",
        "        # softmax is well suited when we predict multiple label for multiple classes:\n",
        "        if activation == 'tanh':\n",
        "            model.add(layers.Dense(nb_classes, activation='tanh'))\n",
        "        elif activation == 'sigmoid':\n",
        "            model.add(layers.Dense(nb_classes, activation='sigmoid'))\n",
        "        else:\n",
        "            raise ValueError(\"activation = \" + activation)\n",
        "\n",
        "        if loss == 'bin_ce':\n",
        "            model.compile(optimizer='adam', loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
        "        elif loss == 'bin_ce-logits':\n",
        "            model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "        elif loss == 'mse':\n",
        "            model.compile(optimizer='adam', loss=tf.keras.losses.mean_squared_error, metrics=['accuracy'])\n",
        "        elif loss == 'jaccard':\n",
        "            model.compile(optimizer='adam', loss=vary_weighted_jaccard, metrics=['accuracy'])\n",
        "        elif loss == 'manhattan':\n",
        "            model.compile(optimizer='adam', loss=vary_manhattan_dist, metrics=['accuracy'])\n",
        "        else:\n",
        "            raise ValueError(\"loss = \" + loss)\n",
        "\n",
        "    else:\n",
        "        # softmax is well suited when we predict a single label for multiple classes:\n",
        "        model.add(layers.Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "        # Categorical Cross Entropy is well suited in this case:\n",
        "        model.compile(optimizer='adam',\n",
        "                      #loss='mean_squared_error',\n",
        "                      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsEihRIvDVh6"
      },
      "source": [
        "### **Training GRU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7oZhk0nDUcG"
      },
      "source": [
        "def get_GRU_model(multi=False, alpha_size=128, nb_classes=1, nb_col=128, nb_unit=10, activation='tanh', loss='mse'):\n",
        "  \n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Embedding(alpha_size, alpha_size, input_length=nb_col,mask_zero=True))\n",
        "    model.add(layers.Bidirectional(layers.GRU(nb_unit, activation='relu', recurrent_activation='sigmoid', reset_after=TRUE,)))\n",
        "\n",
        "    if multi:\n",
        "        if activation == 'tanh':\n",
        "            model.add(layers.Dense(nb_classes, activation='tanh'))\n",
        "        elif activation == 'sigmoid':\n",
        "            model.add(layers.Dense(nb_classes, activation='sigmoid'))\n",
        "        else:\n",
        "            raise ValueError(\"activation = \" + activation)\n",
        "\n",
        "        if loss == 'bin_ce':\n",
        "            model.compile(optimizer='adam', loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
        "        elif loss == 'bin_ce-logits':\n",
        "            model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "        elif loss == 'mse':\n",
        "            model.compile(optimizer='adam', loss=tf.keras.losses.mean_squared_error, metrics=['accuracy'])\n",
        "        elif loss == 'jaccard':\n",
        "            model.compile(optimizer='adam', loss=vary_weighted_jaccard, metrics=['accuracy'])\n",
        "        elif loss == 'manhattan':\n",
        "            model.compile(optimizer='adam', loss=vary_manhattan_dist, metrics=['accuracy'])\n",
        "        else:\n",
        "            raise ValueError(\"loss = \" + loss)\n",
        "\n",
        "    else:\n",
        "        # softmax is well suited when we predict a single label for multiple classes:\n",
        "        model.add(layers.Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "        # Categorical Cross Entropy is well suited in this case:\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW_C-m_RRMmb"
      },
      "source": [
        "### **Training Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJbMnw02VaVa"
      },
      "source": [
        "def get_compiled_model(model_type=\"RNN\", multi=False, alpha_size=128, nb_classes=1, nb_col=128, nb_unit=10, activation='tanh', loss='mse'):\n",
        "    if model_type == \"RNN\":\n",
        "        print(\"Training a RNN\")\n",
        "        model = get_RNN_model(multi, alpha_size, nb_classes, nb_col, nb_unit, activation, loss)\n",
        "    elif model_type == \"LSTM\":\n",
        "        print(\"Training a LSTM\")\n",
        "        model = get_LSTM_model(multi, alpha_size, nb_classes, nb_col, nb_unit, activation, loss)\n",
        "    elif model_type == \"GRU\":\n",
        "        print(\"Training a GRU\")\n",
        "        model = get_GRU_model(multi, alpha_size, nb_classes, nb_col, nb_unit, activation, loss)\n",
        "    else:\n",
        "        sys.exit(\"ERROR: \" + model_type + \" is not recognized as a model type\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def analyze_predictions(multi, predictions, tfclass):\n",
        "    samples = predictions[:15]\n",
        "    np_class = tfclass.numpy()\n",
        "\n",
        "    if multi:\n",
        "        for i in range(0, len(samples)):  # for multi-label we look at the highest predictions\n",
        "            print(\" ============ \" + \"Sample: \" + str(i) + \" ============\")\n",
        "            pred = (-samples[i]).argsort()  # predictions[i]\n",
        "            print(\"pred sorted: \" + str(pred) + \" pred: \" + str(samples[i]))\n",
        "            np_masked_class = ma.masked_equal(np_class[i], 0)\n",
        "            # print(\"masked class: \" + str(np_masked_class))\n",
        "            label = np_masked_class.nonzero()\n",
        "            # print(\"label sorted: \" + str(label[0]) + \" vs \" + str(tfclass[i]) + \" for sample: \" + str(i))\n",
        "            print(\" Top predictions indices: \" + str(pred[:len(label[0])]) + \" vs. real classes indices: \" + str(label[0]))\n",
        "            intersect = np.intersect1d(pred[:len(label[0])], label[0])\n",
        "            union = np.union1d(pred[:len(label[0])], label[0])\n",
        "            jaccard_score = len(intersect) / len(union)\n",
        "            print(f\"Jaccard Score: {jaccard_score}\")\n",
        "            print(\" \")\n",
        "    else:  # for single-label, we only need to retrieve the index of the maximum probability value corresponding to\n",
        "        # the index of the selected class in the ground truth\n",
        "        pred = np.argmax(predictions, axis=1)[:15]\n",
        "        label = np.argmax(tfclass, axis=1)[:15]\n",
        "        # print(tftest[:15])\n",
        "        # print(tfclass[:15])\n",
        "        print(predictions[:15])\n",
        "        print(\"Prediction:      \" + str(pred))\n",
        "        print(\"Expected outcome:\" + str(label))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn5Zho7zVrNw"
      },
      "source": [
        "def main(dataset_filename, ev_encoded, cl_encoded, event2int, int2event, model_type, multi, nb_epochs, nb_unit, batch_size, percent_training, activation, loss):    \n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    # we can give both 0.66 and 66 for instance\n",
        "    if percent_training > 1.0:\n",
        "        percent_training = percent_training / 100\n",
        "\n",
        "    x_tr, x_ts, y_tr, y_ts = train_test_split(ev_encoded, cl_encoded, train_size=percent_training)\n",
        "\n",
        "    print(\"output y_tr\")\n",
        "    # print(y_tr)\n",
        "    print(x_tr.shape)\n",
        "    print(y_tr.shape)\n",
        "    print(x_ts.shape)\n",
        "    print(y_ts.shape)\n",
        "\n",
        "    # Our vectorized labels\n",
        "\n",
        "    print(\"Test generation tensorFlow datasets\")\n",
        "\n",
        "    # turn into tensorFlow dataset\n",
        "    # tf_train = tf.data.Dataset.from_tensor_slices((x_tr.values, y_tr.values))\n",
        "    tf_train = tf.convert_to_tensor(x_tr)\n",
        "    tf_label = tf.convert_to_tensor(y_tr)\n",
        "    # for element in tf_train:\n",
        "    #    print(element)\n",
        "    # tf_test = tf.data.Dataset.from_tensor_slices((x_ts.values, y_ts.values))\n",
        "    tf_test = tf.convert_to_tensor(x_ts)\n",
        "    tf_class = tf.convert_to_tensor(y_ts)\n",
        "\n",
        "    # reshape for tensorflow/keras RNN -> df_dataset.columns = number of features; one class to retrieve\n",
        "    # tf_train = tf.reshape(tf_train,[-1,1,df_dataset.columns])\n",
        "\n",
        "    # tf_test = tf.reshape(tf_test,[-1,1,df_dataset.columns])\n",
        "\n",
        "    print(\"End generating tensorFlow datasets\")\n",
        "\n",
        "    print(\"Alphabet size:\")\n",
        "    print(len(event2int))\n",
        "    model = get_compiled_model(model_type=model_type, multi=multi, alpha_size=len(event2int),\n",
        "                               nb_classes=len(cl_encoded.columns),\n",
        "                               nb_col=len(ev_encoded.columns), nb_unit=nb_unit, activation=activation, loss=loss)\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    callbacks = [TerminateOnNaN()]\n",
        "\n",
        "    history = model.fit(tf_train, tf_label, epochs=nb_epochs, batch_size=batch_size, callbacks=callbacks)\n",
        "\n",
        "    #print('Last loss value:')\n",
        "    #print(list(history.history['loss'])[-1])\n",
        "\n",
        "    print(\"Evaluate on test data\")\n",
        "    results = model.evaluate(tf_test, tf_class, batch_size=batch_size)\n",
        "\n",
        "    print(\"Generate predictions\")\n",
        "    pred_noarg = model.predict(tf_test)\n",
        "    print(\"Analyzing predictions\")\n",
        "    analyze_predictions(multi, pred_noarg, tf_class)\n",
        "\n",
        "    output_directory = drive_dir + 'results/training_metrics/' # \"../../results/training_metrics/\"\n",
        "    output_filename_base = path.basename(dataset_filename)\n",
        "    output_filename = output_filename_base + '_metrics_' + str(model_type) + '_nb_unit_' + str(\n",
        "        nb_unit) + '_training_set_size_' + str(\n",
        "        percent_training) + '_nb_epochs_' + str(nb_epochs) + '_batch_size_' + str(batch_size) + \\\n",
        "                      '_' + TENSORFLOW_DEVICE + '_tensorflow_' + str(loss) + '_' + str(activation)\n",
        "    if multi:\n",
        "        output_filename = output_filename + '_multi.txt'\n",
        "    else:\n",
        "        output_filename = output_filename + '.txt'\n",
        "\n",
        "    output_file = output_directory + output_filename\n",
        "    f = open(output_file, \"a\")\n",
        "    orig_stdout = sys.stdout\n",
        "    sys.stdout = f\n",
        "\n",
        "    seconds = time.time() - start_time\n",
        "    exec_time = time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
        "    results.append(exec_time)\n",
        "\n",
        "    print(\"test loss, test acc, exec time:\", results)\n",
        "\n",
        "    sys.stdout = orig_stdout\n",
        "    f.close()\n",
        "    if np.isnan(results[0]):\n",
        "        print(\"LOSS IS NAN! LOOP AGAIN.\")\n",
        "        main(dataset_filename, model_type, multi, nb_epochs, nb_unit, batch_size, percent_training, activation, loss)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwHB9xjyQJwr"
      },
      "source": [
        "## **Main**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYGcGoUsXtYO"
      },
      "source": [
        "### **Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85pUnDARXoc8"
      },
      "source": [
        "dataset_filename = \"claroline-dis_10.csv\"\n",
        "model_type = \"LSTM\"\n",
        "multi = True\n",
        "nb_epochs = 20\n",
        "nb_unit = 30\n",
        "batch_size = 128\n",
        "percent_training = 0.66 \n",
        "activation = \"tanh\" \n",
        "loss = \"bin_ce\"\n",
        "nb_iterations = 10"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqDhhmfdan9_"
      },
      "source": [
        "### **Checking for GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39PRXhABaoOt",
        "outputId": "36e6ceca-3c7a-4878-f16a-c94f4fffd8e2"
      },
      "source": [
        "if tf_test.is_gpu_available():\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "  if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "  print('GPU device not found, running Tensforflow with CPU')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-28-029f36b4ae2d>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU device not found, running Tensforflow with CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrWHkCa3aW59"
      },
      "source": [
        "### **Loading Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm0y-77CaWRN",
        "outputId": "1b1a3981-3424-4cb4-fbba-35a7b9461a59"
      },
      "source": [
        "print(dataset_filename)\n",
        "ev_encoded, cl_encoded, event2int, int2event = preproc(dataset_filename, multi)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "claroline-dis_10.csv\n",
            "Trying to load: /content/drive/My Drive/VaryMinions-Claroline/claroline-dis_10.csv\n",
            "Categories:\n",
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcacfyEKXp-h"
      },
      "source": [
        "### **Execution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFlAvbuBXonB",
        "outputId": "9baff523-0ac5-454f-bed0-635de1349cd1"
      },
      "source": [
        "print(model_type)\n",
        "for i in range(0, nb_iterations):\n",
        "      print(\"Exécution \" + str(i) + \" : \")\n",
        "      main(dataset_filename, ev_encoded, cl_encoded, event2int, int2event, model_type, multi, nb_epochs, nb_unit, batch_size, percent_training, activation, loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM\n",
            "Exécution 0 : \n",
            "output y_tr\n",
            "(33000, 300)\n",
            "(33000, 10)\n",
            "(17000, 300)\n",
            "(17000, 10)\n",
            "Test generation tensorFlow datasets\n",
            "End generating tensorFlow datasets\n",
            "Alphabet size:\n",
            "50\n",
            "Training a LSTM\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 50)          2500      \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 60)               19440     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                610       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,550\n",
            "Trainable params: 22,550\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "258/258 [==============================] - 163s 603ms/step - loss: 0.5127 - accuracy: 0.1884\n",
            "Epoch 2/20\n",
            " 57/258 [=====>........................] - ETA: 1:59 - loss: 0.2935 - accuracy: 0.5829"
          ]
        }
      ]
    }
  ]
}