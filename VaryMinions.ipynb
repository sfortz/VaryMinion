{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VaryMinions.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNsxbOfA3/z6VsVYIgNTtCN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfortz/VaryMinions/blob/jupyter-notebook/VaryMinions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77MW3TcZgP1E"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGLglA-0gMAm",
        "outputId": "323c43df-1edb-4a1e-e0df-1909f267285f"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import argparse\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import tensorflow as tf\n",
        "import keras  # this activates tensorflow backend directly - do not remove\n",
        "from sklearn.model_selection import train_test_split\n",
        "from os import path\n",
        "from os import environ\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from tensorflow.keras import backend as kb\n",
        "from tensorflow.keras.callbacks import TerminateOnNaN\n",
        "from tensorflow import test as tf_test\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "drive_dir = '/content/drive/My Drive/VaryMinions-Claroline/'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie30xEsiPdlv"
      },
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLvIbfTlMYB9"
      },
      "source": [
        "# load dataset\n",
        "def load_dataset(dataset_filename):\n",
        "    file_to_load = drive_dir + dataset_filename\n",
        "    print(f\"Trying to load: {file_to_load}\")\n",
        "    if not path.exists(drive_dir):\n",
        "        print(f\"The expected dataset directory {drive_dir} was not found.\")\n",
        "        sys.exit()\n",
        "    elif not path.exists(file_to_load):\n",
        "        print(file_to_load)\n",
        "        print(\"The dataset file was not found\")\n",
        "        sys.exit()\n",
        "\n",
        "    # dealing with process of different lengths by filling missing values\n",
        "    dataset = np.genfromtxt(file_to_load, delimiter=',', missing_values='', filling_values='', skip_header=0,\n",
        "                            names=True, dtype=None, encoding=\"utf-8\")\n",
        "    pd_dataset = pd.DataFrame(dataset)\n",
        "    return pd_dataset\n",
        "\n",
        "\n",
        "# retrieve unique values in all dimensions of a dataset -> that would be the 'alphabet' of possible events\n",
        "def retrieve_unique_values(dataset):\n",
        "    flat_dataset = dataset.values.flatten()\n",
        "    unique_value = list(set(flat_dataset))\n",
        "    # print(len(unique_value))\n",
        "    # print(unique_value)\n",
        "    return unique_value\n",
        "\n",
        "\n",
        "# preprocessing dataset\n",
        "def preprocessing_to_num(dataset):\n",
        "    values = retrieve_unique_values(dataset)\n",
        "\n",
        "    # Creating a dictionary that maps integers to the events/actions (adapted from https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)\n",
        "    int2event = dict(enumerate(values))\n",
        "    # Creating another dictionary that maps events/actions to integers (adapted from https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)\n",
        "    event2int = {char: ind for ind, char in int2event.items()}\n",
        "    # print(event2int)\n",
        "\n",
        "    # encoding events with mapping dictionary\n",
        "    encoded_events = []\n",
        "    # print(range(len(dataset)))\n",
        "    for i, row in dataset.iterrows():\n",
        "        encoded_events.append(row)\n",
        "\n",
        "    for i, row in dataset.iterrows():\n",
        "        encoded_events[i] = [event2int[event] for event in row]\n",
        "\n",
        "    return encoded_events, event2int, int2event\n",
        "\n",
        "\n",
        "# preprocessing categories\n",
        "def preprocessing_cat(categories):\n",
        "    lb = preprocessing.LabelBinarizer()\n",
        "    lb.fit(np.array(categories))\n",
        "\n",
        "    print(\"Categories:\")\n",
        "    print(lb.classes_)\n",
        "    cl_enc = lb.transform(categories)\n",
        "\n",
        "    return lb, pd.DataFrame(cl_enc)\n",
        "\n",
        "\n",
        "# preprocessing\n",
        "def preproc(dataset_filename):\n",
        "    df_dataset = load_dataset(dataset_filename)\n",
        "    classes = df_dataset.filter(regex=\"Category.*\")\n",
        "    df_dataset = df_dataset.drop(classes.columns, axis=1)\n",
        "    df_cl_decod, df_cl_encoded = preprocessing_cat(classes)\n",
        "    encoded_ev, event2int, int2event = preprocessing_to_num(df_dataset)\n",
        "    df_ev_encoded = pd.DataFrame(encoded_ev)\n",
        "    return df_ev_encoded, df_cl_encoded, event2int, int2event\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN1iUpE-P-E1"
      },
      "source": [
        "## **Losses**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvLjTHYCP78I"
      },
      "source": [
        "# used for segementation, not sure that what we want here :(\n",
        "def ioU_jaccard_distance(y_true, y_pred, smooth=100):\n",
        "    \"\"\" This distance is copied verbatim from:\n",
        "    https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/jaccard.py\"\"\"\n",
        "\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
        "    print(\"intersection\")\n",
        "    kb.print_tensor(intersection)\n",
        "    print(\"union\")\n",
        "    kb.print_tensor(sum_)\n",
        "\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    return (1 - jac) * smooth\n",
        "\n",
        "\n",
        "def dummy_indicator(y_predicted, thres=0.0):\n",
        "    \"\"\"\n",
        "    This indicator return a tensor with values 0 if values in y_predicted < thres, else 1.\n",
        "    \"\"\"\n",
        "\n",
        "    y_predicted = K.tf.where(K.greater(y_predicted, thres),K.ones_like(y_predicted), K.zeros_like(y_predicted))\n",
        "\n",
        "    return y_predicted\n",
        "\n",
        "# TODO: remove this method not suitable as a loss\n",
        "def vary_boolean_jaccard(y_true, y_predicted):\n",
        "    \"\"\"\n",
        "    This function implements the classic jaccard distance over an indicator functions that converts float predictions\n",
        "    into boolean values.\n",
        "    \"\"\"\n",
        "    y_predicted = dummy_indicator(y_predicted)\n",
        "    y_true = K.cast(y_true, tf.float32)\n",
        "    intersection = K.sum(K.abs(y_true - y_predicted), axis=-1)\n",
        "    union = K.sum(K.ones_like(y_predicted), axis=-1) # number of possible matches (tensors y_predicted, y_true have the same number\n",
        "    # of values)\n",
        "    j_index = K.tf.divide(intersection, union)\n",
        "\n",
        "    return j_index\n",
        "\n",
        "# a different form of the jaccard distance which considers minimum and maximum values of the tensors \n",
        "def vary_weighted_jaccard(y_true, y_predicted):\n",
        "    \"\"\" This function implements the weighted jaccard distance also known as Soergel:\n",
        "    https://en.wikipedia.org/wiki/Jaccard_index. Since it works on real and positive numbers no indicator is needed\n",
        "    to translate probabilities into Boolean values. \"\"\"\n",
        "\n",
        "    # we convert actual labels to float to ease comparisons\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    num = kb.sum(kb.minimum(y_true, y_predicted), axis=-1)\n",
        "    denom = kb.sum(kb.maximum(y_true, y_predicted), axis=-1)\n",
        "    j_index = tf.divide(num, denom)\n",
        "\n",
        "    ones = tf.ones_like(j_index)\n",
        "    return ones - j_index\n",
        "\n",
        "# bounded version of the weighted jaccard distance putting negative values to 0\n",
        "def vary_weighted_jaccard_rectified(y_true, y_predicted):\n",
        "    \"\"\" This function implements the weighted jaccard distance also known as Soergel:\n",
        "    https://en.wikipedia.org/wiki/Jaccard_index. Since it works on real and positive numbers no indicator is needed\n",
        "    to translate probabilities into Boolean values. This function \"rectifies\" negative values to zero. \"\"\"\n",
        "\n",
        "    # rectification of negative values to zero\n",
        "    zeros = tf.zeros_like(y_predicted)\n",
        "    y_predicted = kb.maximum(y_predicted, zeros)\n",
        "    # we convert actual labels to float to ease comparisons\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "\n",
        "    num = kb.sum(kb.minimum(y_true, y_predicted), axis=-1)\n",
        "    denom = kb.sum(kb.maximum(y_true, y_predicted), axis=-1)\n",
        "    j_index = tf.divide(num, denom)\n",
        "\n",
        "    ones = tf.ones_like(j_index)\n",
        "    return ones - j_index\n",
        "\n",
        "# the manhattan distance between two tensors\n",
        "def vary_manhattan_dist(y_true,y_predicted):\n",
        "    \"\"\"\n",
        "    Implements Manatthan distance a loss\n",
        "    \"\"\"\n",
        "    y_actual_float = tf.cast(y_true, tf.float32)\n",
        "    manh_dist = kb.sum(kb.abs(y_actual_float - y_predicted), axis=-1)\n",
        "    return manh_dist\n",
        "\n",
        "# TODO remove this method\n",
        "def vary_manhattan_dist_indiv(y_true,y_predicted):\n",
        "    \"\"\"\n",
        "    Implements Manatthan distance a loss. Reports detailed losses per classes. DO NOT USE\n",
        "    \"\"\"\n",
        "    y_actual_float = tf.cast(y_true, tf.float32)\n",
        "    manh_dist = kb.abs(y_actual_float - y_predicted)\n",
        "    return manh_dist\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVYgkVYEObSw"
      },
      "source": [
        "## **Training Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acKoqTl5PriL"
      },
      "source": [
        "### **Training LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNROM35APqMf"
      },
      "source": [
        "def get_LSTM_model(alpha_size=128, nb_classes=1, nb_col=128, nb_unit=10, activation='tanh', loss='mse'):\n",
        "    \n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Embedding(alpha_size, alpha_size, mask_zero=True))\n",
        "    #model.add(layers.Bidirectional(layers.LSTM(nb_unit, activation='relu', recurrent_activation = 'sigmoid')))\n",
        "    model.add(layers.Bidirectional(layers.LSTM(nb_unit, activation='tanh', recurrent_activation = 'sigmoid', use_bias=True, recurrent_dropout=0.0, unroll=False)))\n",
        "###\n",
        "   \n",
        "   # softmax is well suited when we predict multiple label for multiple classes:\n",
        "    if activation == 'tanh':\n",
        "        model.add(layers.Dense(nb_classes, activation='tanh'))\n",
        "    elif activation == 'sigmoid':\n",
        "        model.add(layers.Dense(nb_classes, activation='sigmoid'))\n",
        "    else:\n",
        "        raise ValueError(\"activation = \" + activation)\n",
        "\n",
        "    if loss == 'bin_ce':\n",
        "        model.compile(optimizer='adam', loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
        "    elif loss == 'bin_ce-logits':\n",
        "        model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "    elif loss == 'mse':\n",
        "        model.compile(optimizer='adam', loss=tf.keras.losses.mean_squared_error, metrics=['accuracy'])\n",
        "    elif loss == 'jaccard':\n",
        "        model.compile(optimizer='adam', loss=vary_weighted_jaccard, metrics=['accuracy'])\n",
        "    elif loss == 'manhattan':\n",
        "        model.compile(optimizer='adam', loss=vary_manhattan_dist, metrics=['accuracy'])\n",
        "    else:\n",
        "        raise ValueError(\"loss = \" + loss)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsEihRIvDVh6"
      },
      "source": [
        "### **Training GRU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7oZhk0nDUcG"
      },
      "source": [
        "def get_GRU_model(alpha_size=128, nb_classes=1, nb_col=128, nb_unit=10, activation='tanh', loss='mse'):\n",
        "  \n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Embedding(alpha_size, alpha_size, input_length=nb_col,mask_zero=True))\n",
        "    model.add(layers.Bidirectional(layers.GRU(nb_unit, activation='relu', recurrent_activation='sigmoid', reset_after=TRUE,)))\n",
        "\n",
        "    if activation == 'tanh':\n",
        "        model.add(layers.Dense(nb_classes, activation='tanh'))\n",
        "    elif activation == 'sigmoid':\n",
        "        model.add(layers.Dense(nb_classes, activation='sigmoid'))\n",
        "    else:\n",
        "        raise ValueError(\"activation = \" + activation)\n",
        "\n",
        "    if loss == 'bin_ce':\n",
        "        model.compile(optimizer='adam', loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
        "    elif loss == 'bin_ce-logits':\n",
        "        model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "    elif loss == 'mse':\n",
        "        model.compile(optimizer='adam', loss=tf.keras.losses.mean_squared_error, metrics=['accuracy'])\n",
        "    elif loss == 'jaccard':\n",
        "        model.compile(optimizer='adam', loss=vary_weighted_jaccard, metrics=['accuracy'])\n",
        "    elif loss == 'manhattan':\n",
        "        model.compile(optimizer='adam', loss=vary_manhattan_dist, metrics=['accuracy'])\n",
        "    else:\n",
        "        raise ValueError(\"loss = \" + loss)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW_C-m_RRMmb"
      },
      "source": [
        "### **Training Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJbMnw02VaVa"
      },
      "source": [
        "def get_compiled_model(model_type=\"RNN\", alpha_size=128, nb_classes=1, nb_col=128, nb_unit=10, activation='tanh', loss='mse'):\n",
        "    if model_type == \"RNN\":\n",
        "        print(\"Training a RNN\")\n",
        "        model = get_RNN_model(alpha_size, nb_classes, nb_col, nb_unit, activation, loss)\n",
        "    elif model_type == \"LSTM\":\n",
        "        print(\"Training a LSTM\")\n",
        "        model = get_LSTM_model(alpha_size, nb_classes, nb_col, nb_unit, activation, loss)\n",
        "    elif model_type == \"GRU\":\n",
        "        print(\"Training a GRU\")\n",
        "        model = get_GRU_model(alpha_size, nb_classes, nb_col, nb_unit, activation, loss)\n",
        "    else:\n",
        "        sys.exit(\"ERROR: \" + model_type + \" is not recognized as a model type\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def analyze_predictions(predictions, tfclass):\n",
        "    samples = predictions[:15]\n",
        "    np_class = tfclass.numpy()\n",
        "\n",
        "    for i in range(0, len(samples)):  # for multi-label we look at the highest predictions\n",
        "        print(\" ============ \" + \"Sample: \" + str(i) + \" ============\")\n",
        "        pred = (-samples[i]).argsort()  # predictions[i]\n",
        "        print(\"pred sorted: \" + str(pred) + \" pred: \" + str(samples[i]))\n",
        "        np_masked_class = ma.masked_equal(np_class[i], 0)\n",
        "        label = np_masked_class.nonzero()\n",
        "        print(\" Top predictions indices: \" + str(pred[:len(label[0])]) + \" vs. real classes indices: \" + str(label[0]))\n",
        "        intersect = np.intersect1d(pred[:len(label[0])], label[0])\n",
        "        union = np.union1d(pred[:len(label[0])], label[0])\n",
        "        jaccard_score = len(intersect) / len(union)\n",
        "        print(f\"Jaccard Score: {jaccard_score}\")\n",
        "        print(\" \")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn5Zho7zVrNw"
      },
      "source": [
        "def main(dataset_filename, ev_encoded, cl_encoded, event2int, int2event, model_type, nb_epochs, nb_unit, batch_size, percent_training, activation, loss):    \n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    # we can give both 0.66 and 66 for instance\n",
        "    if percent_training > 1.0:\n",
        "        percent_training = percent_training / 100\n",
        "\n",
        "    x_tr, x_ts, y_tr, y_ts = train_test_split(ev_encoded, cl_encoded, train_size=percent_training)\n",
        "\n",
        "    print(\"output y_tr\")\n",
        "    # print(y_tr)\n",
        "    print(x_tr.shape)\n",
        "    print(y_tr.shape)\n",
        "    print(x_ts.shape)\n",
        "    print(y_ts.shape)\n",
        "\n",
        "    # Our vectorized labels\n",
        "\n",
        "    print(\"Test generation tensorFlow datasets\")\n",
        "\n",
        "    # turn into tensorFlow dataset\n",
        "    # tf_train = tf.data.Dataset.from_tensor_slices((x_tr.values, y_tr.values))\n",
        "    tf_train = tf.convert_to_tensor(x_tr)\n",
        "    tf_label = tf.convert_to_tensor(y_tr)\n",
        "    # for element in tf_train:\n",
        "    #    print(element)\n",
        "    # tf_test = tf.data.Dataset.from_tensor_slices((x_ts.values, y_ts.values))\n",
        "    tf_test = tf.convert_to_tensor(x_ts)\n",
        "    tf_class = tf.convert_to_tensor(y_ts)\n",
        "\n",
        "    # reshape for tensorflow/keras RNN -> df_dataset.columns = number of features; one class to retrieve\n",
        "    # tf_train = tf.reshape(tf_train,[-1,1,df_dataset.columns])\n",
        "\n",
        "    # tf_test = tf.reshape(tf_test,[-1,1,df_dataset.columns])\n",
        "\n",
        "    print(\"End generating tensorFlow datasets\")\n",
        "\n",
        "    print(\"Alphabet size:\")\n",
        "    print(len(event2int))\n",
        "    model = get_compiled_model(model_type=model_type, alpha_size=len(event2int),\n",
        "                               nb_classes=len(cl_encoded.columns),\n",
        "                               nb_col=len(ev_encoded.columns), nb_unit=nb_unit, activation=activation, loss=loss)\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    callbacks = [TerminateOnNaN()]\n",
        "\n",
        "    history = model.fit(tf_train, tf_label, epochs=nb_epochs, batch_size=batch_size, callbacks=callbacks)\n",
        "\n",
        "    #print('Last loss value:')\n",
        "    #print(list(history.history['loss'])[-1])\n",
        "\n",
        "    print(\"Evaluate on test data\")\n",
        "    results = model.evaluate(tf_test, tf_class, batch_size=batch_size)\n",
        "\n",
        "    print(\"Generate predictions\")\n",
        "    pred_noarg = model.predict(tf_test)\n",
        "    print(\"Analyzing predictions\")\n",
        "    analyze_predictions(pred_noarg, tf_class)\n",
        "\n",
        "    output_directory = drive_dir + 'results/training_metrics/' # \"../../results/training_metrics/\"\n",
        "    output_filename_base = path.basename(dataset_filename)\n",
        "    output_filename = output_filename_base + '_metrics_' + str(model_type) + '_nb_unit_' + str(\n",
        "        nb_unit) + '_training_set_size_' + str(\n",
        "        percent_training) + '_nb_epochs_' + str(nb_epochs) + '_batch_size_' + str(batch_size) + \\\n",
        "                      '_' + TENSORFLOW_DEVICE + '_tensorflow_' + str(loss) + '_' + str(activation)\n",
        "  \n",
        "    output_filename = output_filename + '_multi.txt'\n",
        "\n",
        "    output_file = output_directory + output_filename\n",
        "    f = open(output_file, \"a\")\n",
        "    orig_stdout = sys.stdout\n",
        "    sys.stdout = f\n",
        "\n",
        "    seconds = time.time() - start_time\n",
        "    exec_time = time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
        "    results.append(exec_time)\n",
        "\n",
        "    print(\"test loss, test acc, exec time:\", results)\n",
        "\n",
        "    sys.stdout = orig_stdout\n",
        "    f.close()\n",
        "    if np.isnan(results[0]):\n",
        "        print(\"LOSS IS NAN! LOOP AGAIN.\")\n",
        "        main(dataset_filename, model_type, nb_epochs, nb_unit, batch_size, percent_training, activation, loss)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwHB9xjyQJwr"
      },
      "source": [
        "## **Main**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYGcGoUsXtYO"
      },
      "source": [
        "### **Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85pUnDARXoc8"
      },
      "source": [
        "dataset_filename = \"claroline-dis_10.csv\"\n",
        "model_type = \"LSTM\"\n",
        "nb_epochs = 20\n",
        "nb_unit = 30\n",
        "batch_size = 128\n",
        "percent_training = 0.66 \n",
        "activation = \"tanh\" \n",
        "loss = \"bin_ce\"\n",
        "nb_iterations = 10"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqDhhmfdan9_"
      },
      "source": [
        "### **Checking for GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39PRXhABaoOt",
        "outputId": "7b4c8394-8621-4603-924e-e37ccb07a0c5"
      },
      "source": [
        "if tf_test.is_gpu_available():\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "  if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "  !nvidia-smi -L\n",
        "else:\n",
        "  print('GPU device not found, running Tensforflow with CPU')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "GPU 0: Tesla K80 (UUID: GPU-65db67d6-48b7-62c5-6382-59f9df755112)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrWHkCa3aW59"
      },
      "source": [
        "### **Loading Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm0y-77CaWRN",
        "outputId": "8b4216e3-c409-418d-d16f-29c3dbc19ebe"
      },
      "source": [
        "print(dataset_filename)\n",
        "ev_encoded, cl_encoded, event2int, int2event = preproc(dataset_filename)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "claroline-dis_10.csv\n",
            "Trying to load: /content/drive/My Drive/VaryMinions-Claroline/claroline-dis_10.csv\n",
            "Categories:\n",
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcacfyEKXp-h"
      },
      "source": [
        "### **Execution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TFlAvbuBXonB",
        "outputId": "57e8f3d7-6ae8-4e8f-af87-dc7280a7e49c"
      },
      "source": [
        "print(model_type)\n",
        "for i in range(0, nb_iterations):\n",
        "      print(\"Exécution \" + str(i) + \" : \")\n",
        "      main(dataset_filename, ev_encoded, cl_encoded, event2int, int2event, model_type, nb_epochs, nb_unit, batch_size, percent_training, activation, loss)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM\n",
            "Exécution 0 : \n",
            "output y_tr\n",
            "(33000, 300)\n",
            "(33000, 10)\n",
            "(17000, 300)\n",
            "(17000, 10)\n",
            "Test generation tensorFlow datasets\n",
            "End generating tensorFlow datasets\n",
            "Alphabet size:\n",
            "50\n",
            "Training a LSTM\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 50)          2500      \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 60)               19440     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                610       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,550\n",
            "Trainable params: 22,550\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "258/258 [==============================] - 556s 2s/step - loss: 0.5692 - accuracy: 0.1165\n",
            "Epoch 2/20\n",
            "258/258 [==============================] - 546s 2s/step - loss: 0.4294 - accuracy: 0.6532\n",
            "Epoch 3/20\n",
            "258/258 [==============================] - 531s 2s/step - loss: 0.6302 - accuracy: 0.5967\n",
            "Epoch 4/20\n",
            "219/258 [========================>.....] - ETA: 1:17 - loss: 0.6246 - accuracy: 0.5965"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ae09920c2012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exécution \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m       \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mev_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent2int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint2event\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_unit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-1038681449d2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dataset_filename, ev_encoded, cl_encoded, event2int, int2event, model_type, nb_epochs, nb_unit, batch_size, percent_training, activation, loss)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTerminateOnNaN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#print('Last loss value:')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlDcmitOMQst"
      },
      "source": [
        "## Archives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwGzwrRtWJ8x",
        "outputId": "f58269bc-dfeb-4050-8380-5e25dab0518b"
      },
      "source": [
        "file1 = ev_encoded.to_numpy()\n",
        "print(\"ev_encoded:\")\n",
        "print(file1)\n",
        "file2 = cl_encoded.to_numpy()\n",
        "print(\"cl_encoded:\")\n",
        "print(file2)\n",
        "print(\"event2int:\")\n",
        "print(event2int)\n",
        "#file3 = event2int.to_numpy()\n",
        "#file4 = int2event.to_numpy()\n",
        "storing_dataset(\"ev_encoded\", file1)\n",
        "storing_dataset(\"cl_encoded\", file2)\n",
        "#storing_dataset(\"event2int\", file3)\n",
        "#storing_dataset(\"int2event\", file4)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ev_encoded:\n",
            "[[49 32 32 ...  1  1  1]\n",
            " [49 32 32 ...  1  1  1]\n",
            " [49 32 32 ...  1  1  1]\n",
            " ...\n",
            " [49 32 32 ...  1  1  1]\n",
            " [49 32 32 ...  1  1  1]\n",
            " [49 32 32 ...  1  1  1]]\n",
            "cl_encoded:\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]]\n",
            "event2int:\n",
            "{'': 0, False: 1, 'clic(/claroline/tracking/delete_course_stats.php)': 2, 'clic(/claroline/learnPath/learningPath.php)': 3, 'clic(/claroline/phpbb/viewforum.php)': 4, 'clic(/claroline/phpbb/index.php)': 5, 'clic(/claroline/exercise/exercise.php)': 6, 'clic(/claroline/wiki/page.php)': 7, 'clic(/claroline/announcements/announcements.php)': 8, 'clic(/claroline/learnPath/module.php)': 9, 'clic(/claroline/user/user.php)': 10, 'clic(/claroline/messaging/readmessage.php)': 11, 'clic(/claroline/auth/login.php)': 12, 'clic(/claroline/tracking/course_access_details.php)': 13, 'clic(/claroline/calendar/agenda.php)': 14, 'clic(/claroline/tracking/userReport.php)': 15, 'clic(/claroline/wiki/wiki.php)': 16, 'clic(/claroline/module/CLCHAT/index.php)': 17, 'clic(/claroline/auth/profile.php)': 18, 'clic(/claroline/learnPath/learningPathAdmin.php)': 19, 'clic(/claroline/learnPath/insertMyModule.php)': 20, 'clic(0)': 21, 'clic(/claroline/user/class_add.php)': 22, 'clic(/claroline/course/create.php)': 23, 'clic(/claroline/learnPath/insertMyDoc.php)': 24, 'clic(/claroline/group/group.php)': 25, 'clic(/claroline/document/document.php)': 26, 'clic(/claroline/admin/managing/editFile.php)': 27, 'clic(/claroline/tracking/courseReport.php)': 28, 'clic(/claroline/phpbb/viewtopic.php)': 29, 'clic(/claroline/user/user_pictures.php)': 30, 'clic(/claroline/auth/courses.php)': 31, 'clic(/claroline/index.php)': 32, 'clic(/claroline/course/settings.php)': 33, 'clic(/claroline/desktop/index.php)': 34, 'clic(/claroline/user/userInfo.php)': 35, 'clic(/claroline/course_description/index.php)': 36, 'clic(/claroline/course/index.php)': 37, 'clic(/claroline/work/work.php)': 38, 'clic(/claroline/user/user_add.php)': 39, 'clic(/claroline/notification_date.php)': 40, 'clic(/claroline/learnPath/insertMyExercise.php)': 41, 'clic(/claroline/messaging/messagebox.php)': 42, 'clic(/claroline/messaging/sendmessage.php)': 43, 'clic(/claroline/course/tools.php)': 44, 'clic(/claroline/admin/index.php)': 45, 'clic(/claroline/user/addcsvusers.php)': 46, 'clic(/claroline/learnPath/learningPathList.php)': 47, 'clic(/claroline/backends/download.php)': 48, 'clic(/claroline/admin/admin_category.php)': 49}\n",
            "Type: \n",
            "int64\n",
            "<HDF5 dataset \"ev_encoded\": shape (50000, 300), type \"<i8\">\n",
            "Type: \n",
            "int64\n",
            "<HDF5 dataset \"cl_encoded\": shape (50000, 10), type \"<i8\">\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB6nKro-XBpd"
      },
      "source": [
        "import h5py\n",
        "\n",
        "def storing_dataset(fileName, fileData):\n",
        "    # Create a new HDF5 file\n",
        "    with h5py.File(drive_dir + fileName + '.h5', 'w') as file:\n",
        "      # Create a dataset in the file\n",
        "        print(\"Type: \")\n",
        "        #arr = ev_encoded.to_numpy()\n",
        "        print(fileData.dtype) \n",
        "        #print(image.astype('<U50').dtype)\n",
        "\n",
        "        dataset = file.create_dataset(fileName, np.shape(fileData), h5py.h5t.NATIVE_INT64, data=fileData)\n",
        "        print(dataset)\n",
        "        #meta_set = file.create_dataset(\"meta\", np.shape(label), h5py.h5t.STD_U8BE, data=label)"
      ],
      "execution_count": 47,
      "outputs": []
    }
  ]
}